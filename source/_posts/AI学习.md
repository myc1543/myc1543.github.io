---
title: AI学习
typora-root-url: AI学习
date: 2025-12-28 20:44:45
tags: [RAG]
categories: [技术,AI]
description: 关于大模型RAG的学习
---

2025年，AI的热度越来越火，许多公司的业务都在朝大模型应用发展，校招面试和社招面试也会提问大模型相关的知识，对于后端选手来说，学习AI相关的知识是必然的。

## 1.术语解释

- AI：Artificial Intelligence的缩写，指“人工智能”，人工智能是指模拟人类智能的计算机系统或软件，使其能够执行诸如学习、推理、问题解决、感知、语言理解等复杂任务。
- 生成式AI：是一种人工智能技术，能够自动生成新的内容，如文本、图像、音频和视频等。与传统的AI不同，生成式AI不仅能分析和理解数据，还能基于其学习到的信息创造出新的内容。
- AIGC：AI Generated Content的缩写，意指由人工智能生成的内容。在算法和数码内容制作领域，AIGC 涉及使用人工智能技术生成各种形式的内容，比如文字、图像、视频、音乐等。

- NLP：Natural Language Processing的缩写，指“自然语言处理”，自然语言处理是人工智能的一个子领域，主要研究计算机如何理解、解释和生成人类语言。NLP技术包括文本分析、语言生成、机器翻译、情感分析、对话系统等。
- LLM：Large Language Model的缩写，指“大语言模型”，这类模型是基于机器学习和深度学习技术，特别是自然语言处理（NLP）中的一种技术。大语言模型通过大量的文本数据进行训练，以生成、理解和处理自然语言。一些著名的 LLM 示例包括 OpenAI 的 GPT（Generative Pre-trained Transformer）系列模型，如 GPT-3 和 GPT-4
- RAG：Retrieval-Augmented Generation的缩写，指“检索增强生成”，这是一个跨越检索和生成任务的框架，通过先从数据库或文档集合中检索到相关信息，然后利用生成模型（如Transformer模型）来生成最终的输出。目前在技术发展趋势和应用落地上，RAG是工程同学较为值得探索的领域。
- Agent：中文叫智能体，一个能独立执行任务和做出决策的实体，在人工智能中，Agent可以是一个机器人，一个虚拟助手，或是一个智能软件系统，它能够通过学习和推理来完成复杂任务。在多Agent系统中，多个独立的Agents相互协作或竞争，以共同解决问题或完成任务。

- GPT：Generative Pre-trained Transformer的缩写，指“生成式预训练变换器”，GPT 模型利用大量文本数据进行预训练，然后可以通过微调来执行特定任务，例如语言生成、回答问题、翻译、文本摘要等。
- chatGPT：由 OpenAI 开发的一种基于 GPT（生成预训练变换模型）架构的人工智能聊天机器人。它使用自然语言处理技术，能够理解并生成类似人类的文本回复。可以看做是一种Agent。
- Prompt：指的是提供给模型的一段初始文本，用于引导模型生成后续的内容。
- Embedding：中文叫嵌入，是一种将高维数据映射到低维空间的技术，但仍尽可能保留原数据的特征和结构。嵌入技术通常用于处理和表示复杂的数据如文本、图像、音乐以及其他高维度的数据类型。



## 2.LLM

[**大型语言模型**](https://zh.wikipedia.org/wiki/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B)（英语：large language model，LLM），也称**大语言模型**，简称**大模型**，是一种基于[人工神经网络](https://zh.wikipedia.org/wiki/人工神经网络)的[语言模型](https://zh.wikipedia.org/wiki/语言模型)。其名称中的“大型”指模型具有庞大的参数量（通常在数十亿至数万亿级别，如GPT-3含1750亿参数）以及巨大的训练数据规模。大语言模型通常采用[自监督机器学习](https://zh.wikipedia.org/wiki/自监督学习)方法，从而能够基于海量无标注的文本进行训练。大语言模型专为[自然语言处理](https://zh.wikipedia.org/wiki/自然语言处理)任务而设计，尤其适用于[语言生成](https://zh.wikipedia.org/wiki/自然语言生成)。



### 2.1 大模型的应用

![](640)

一个典型的大模型应用架构如上图所示，其实和我们平时开发的应用没什么两样。我们平时开发应用，也是处理用户请求，然后调用其它服务实现具体功能。在这个图中，大模型也就是一个普通的下游服务。

#### 2.1.1  联网搜索

大模型的回答是基于训练的数据，如果提问的内容不在训练的知识范围内，那么大模型只能进行推理，自身没有能力获取外界的知识。联网搜索就是把互联网的内容提供给大模型，让大模型能够参考搜索的结果，给出回答。

![](640)

####  2.1.2  Prompt Engineering

提示词工程（Prompt Engineering）是一种在人工智能和自然语言处理领域中开发和设计提示词（Prompts）以引导大型语言模型（例如GPT-3等）产生特定输出的方法。通过精心构建和优化提示词，用户可以更有效地获得所需的答案、生成文本或执行其他自然语言处理任务。

提示词可以从大模型中获取大量结果，但是这个结果不一定是期望的结果，所以可以优化提示词，提供更多的信息以及完善度，得到质量更高的结果

**举个例子**

比如我们想完成对句子的续写：天空是？

- 不添加提示词直接询问

![image-20251228213141341](image-20251228213141341.png)

结果并不是我们想要的



- 添加提示词

![image-20251228213304892](image-20251228213304892.png)

这里明确告诉大模型，我们需要对句子进行续写，生成的质量就比刚才高很多。



#### 2.1.3 提示词要素

**指令**：想要模型执行的特定任务或指令。

**上下文**：包含外部信息或额外的上下文信息，引导语言模型更好地响应。

**输入数据**：用户输入的内容或问题。

**输出指示**：指定输出的类型或格式。



#### 2.1.4 设计提示词的技巧

- 指令
  使用命令来指示模型执行各种简单任务，例如“写入”、“分类”、“总结”、“翻译”、“排序”等，从而为各种简单任务设计有效的提示。通常，上下文越具体和跟任务越相关则效果越好
- 具体性
  要非常具体地说明你希望模型执行的指令和任务。提示越具描述性和详细，结果越好。特别是当你对生成的结果或风格有要求时，这一点尤为重要。不存在什么特定的词元（tokens）或关键词（tokens）能确定带来更好的结果。更重要的是要有一个具有良好格式和描述性的提示词。事实上，在提示中提供示例对于获得特定格式的期望输出非常有效。
  在设计提示时，还应注意提示的长度，因为提示的长度是有限制的。想一想你需要多么的具体和详细。包含太多不必要的细节不一定是好的方法。这些细节应该是相关的，并有助于完成手头的任务。这是你需要进行大量实验的事情。我们鼓励大量实验和迭代，以优化适用于你应用的提示。
- 避免不明确
  给定上述关于详细描述和改进格式的建议，很容易陷入陷阱：想要在提示上过于聪明，从而可能创造出不明确的描述。通常来说，具体和直接会更好。这里的类比非常类似于有效沟通——越直接，信息传达得越有效。
- 做什么还是不做什么
  设计提示时的另一个常见技巧是避免说不要做什么，而应该说要做什么。这样（说要做什么）更加的具体，并且聚焦于（有利于模型生成良好回复的）细节上。

#### 2.1.5 提示技术

提示技术是更高级的技术，完成更加复杂的任务

- 零样本提示
- 少样本提示
- 链式思考(COT)提示
- 生成知识提示

### 2.2 LLM的问题

#### 2.2.1 知识截止

LLM 训练不是实时的，而是离线训练好的。在训练过程中，使用的数据都是提前准备的，而且大多数是公开、开源的数据，这就导致了 LLM 训练后具备的知识是有范围的。换句话说，**模型知识仅限于训练数据所涵盖的知识范围，**对于新的知识（比如今天的新闻）或未训练的知识（比如未公开的数据），模型本身不具备这些知识，仅具备推理能力。

#### 2.2.2 幻觉现象

幻觉现象有多种维度的解释。一方面，LLM 是一个条件概率模型，以前文作为条件的词表概率逐词生成文本，这一机制导致其可能出现**看似逻辑严谨（概率高）但其实缺乏事实依据的生成，**也就是“一本正经地胡说八道”。另一方面，LLM 的训练过程，是**对训练数据的知识进行压缩提炼的过程，但不是无损压缩知识，**边缘知识容易在主流知识冲击下出现扭曲，导致产生了幻觉。



## 3.RAG

针对LLM的问题，RAG可以有效解决模型知识截止和幻觉现象。RAG 是检索增强生成（Retrieval-Augmented Generation）的缩写。检索增强生成，是指对大语言模型（LLM）输入进行优化，使其能够在生成响应之前引用训练数据来源之外的知识，作为回答的根据。这是一种经济高效地改进 LLM 输出的方法，让 LLM 保持相关性、准确性和实用性。

### 3.1 RAG的组成部分

- **检索（Retrieval）：**查询外部数据源，例如知识库、向量数据库或者网页搜索API。常见的检索方法有全文检索、向量检索、图检索等。
- **生成（Generation）：**将检索信息提供给 LLM，生成回答。

### 3.2 RAG的应用

具体来说，RAG模型在生成答案之前，会首先从一个大型的文档库或知识库中检索到若干条相关的文档片段。再将这些检索到的片段作为额外的上下文信息，输入到生成模型中，从而生成更为准确和信息丰富的文本。

RAG的工作原理可以分为以下几个步骤：

1.接收请求：首先，系统接收到用户的请求（例如提出一个问题）。

2.信息检索（R)：系统从一个大型文档库中检索出与查询最相关的文档片段。这一步的目标是找到那些可能包含答案或相关信息的文档。

3.生成增强（A）：将检索到的文档片段与原始查询一起输入到大模型（如chatGPT）中，注意使用合适的提示词，比如原始的问题是XXX，检索到的信息是YYY，给大模型的输入应该类似于：请基于YYY回答XXXX。

4.输出生成（G)：大模型基于输入的查询和检索到的文档片段生成最终的文本答案，并返回给用户。

RAG 需要注意两个问题：

- 检索结果 和 解答问题需要参考的资料 越相关，生成结果越准确。
- 检索出过多的内容，又会引入更多的噪声，影响 LLM 注意力，增加幻觉风险，生成的质量反而降低。

为了尽可能准确地找到和原始问题相关的内容，我们需要某种程度上尽可能 **理解原问题的语义。**

### 3.3 向量数据库

在R阶段，需要理解用户的输入，也就是用户输入的语义。

向量数据库可以解决语义的问题。举个例子，比如“老婆”和“妻子”，这两个词是不同的，但是从语义上来说，它们是相同的。向量数据库使用高维向量来表示一个词的语义。语义越相似的文本，在向量空间中的位置越相近；语义差异越大，在向量空间中的距离越远。